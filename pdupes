#!/usr/bin/perl -w
use warnings;
use strict;

=head1 NAME

pdupes - Perl duplicate file finder and deduper 

=cut

use File::Find;
use Digest::xxHash qw(xxhash64);
use Data::Dumper;

use constant {
    LOG_INFO => 0,
    
    HASH_SEED => 0xc001c0de,
    SHORT_HASH_LENGTH => 1024
};

=head1 SYNOPSIS

    pdupes <directory> [<directory> ...]

=head1 DESCRIPTION

pdupes will find duplicate files with identical file content and try
to hardlink them together to save disk space.

=cut
    
die "no directories given" unless @ARGV;
my @dirs_to_scan = @ARGV;


# print info text (for debugging)
sub info
{
    return unless LOG_INFO;
    print "@_\n";
}

# print formatted info text (for debugging)
sub infof
{
    return unless LOG_INFO;
    
    my $fmt = shift @_;
    info(sprintf($fmt, @_));
}

# compute and show statistics of global file hash
sub dump_stats
{
    return unless LOG_INFO;

    my ($files) = (@_);
    
    my ($dev_count, $size_count, $file_count) = (0, 0, 0);
    foreach my $dev (keys %{$files}) {
	$dev_count++;
	foreach my $size (keys %{$files->{$dev}}) {
	    $size_count++;
	    $file_count += keys @{$files->{$dev}->{$size}};
	}
    }

    infof '%4d devices %12d sizes %12d files', $dev_count, $size_count, $file_count;
}

# compute and show statistics hashed files
sub dump_hash_stats
{
    return unless LOG_INFO;
    
    my ($hashes) = (@_);
    my ($hash_count, $max_dupes) = (0, 0);

    foreach my $hash (keys %{$hashes}) {
	$hash_count++;
	my $dupes = @{$hashes->{$hash}};
	if ($dupes > $max_dupes) {
	    $max_dupes = $dupes;
	}
    }

    infof '%8d hashes, %8d maximum dupes', $hash_count, $max_dupes;
}

# return the smaller of two values
sub min
{
    my ($a, $b) = (@_);
    if ($a < $b) {
	return $a;
    }
    else {
	return $b;
    }
}

# compute the hash of a file up to a given length
sub compute_hash
{
    my ($file, $maxlen) = (@_);

    my $hash = HASH_SEED;
    my $data;
    
    open HASH, '<', $file->{NAME} or die "can't open `$file->{NAME}': $!";
    my $rest = $file->{SIZE};
    while ($rest) {
	my $to_read = min($rest, $file->{BLKSIZE});
	my $read = read(HASH, $data, $to_read);
	die "read $read bytes, but wanted to read $to_read on `$file->{NAME}': $!" unless $to_read = $read;
	$hash = xxhash64($data, $hash);
	$rest -= $read;
    }
    close HASH or die "can't close `$file->{NAME}': $!";

    return $hash;
}

# compute the short hash of a file
sub compute_short_hash
{
    my ($file) = (@_);

    return compute_hash($file, min($file->{SIZE}, SHORT_HASH_LENGTH));
}

# compute the long hash of a file
sub compute_long_hash
{
    my ($file) = (@_);

    return compute_hash($file, $file->{SIZE});
}

# removes lists with only 1 list element from a hash of lists
sub remove_single_entries
{
    my ($hashOfLists) = (@_);

    foreach my $hashKey (keys %{$hashOfLists}) {
	if (@{$hashOfLists->{$hashKey}} == 1) {
	    delete $hashOfLists->{$hashKey};
	}
    }
}

=head1 DETAILED MODE OF OPERATION

pdupes recursively scans all given directories for normal files.
Symlinks, devices, named pipes etc. are skipped.  Empty files are
skipped as well.

Files are grouped by the device their filesystem is on (as hardlinks
can never span multiple filesystems) and their file size (because
identical files must have identical filesizes).

=cut

my $allfiles;
sub register_file
{
    my ($dev, $inode, $mode, $nlink, $uid, $gid, undef, $size, $atime, $mtime, $ctime, $blksize) = lstat($_);

    return unless -f _; # only care about normal files
    return unless $size; # skip empty files

    my $file = {
	DEV => $dev,
	INODE => $inode,
	MODE => $mode,
	NLINK => $nlink,
	UID => $uid,
	GID => $gid,
	SIZE => $size,
	MTIME => $mtime,
	CTIME => $ctime,
	BLKSIZE => $blksize,
	NAME => $File::Find::name
    };

    push @{$allfiles->{$dev}->{$size}}, $file;
}

find(\&register_file, @dirs_to_scan);
dump_stats($allfiles);

=pod

File size groups containing only a single file (a unique file size)
are discarded, because there can't be any duplicates.

=cut

sub remove_single_sizes
{
    foreach my $dev (keys %{$allfiles}) {
	remove_single_entries($allfiles->{$dev});
    }
}

=pod

The files of every device/size-group are read and a partial hash value
over the beginning of the file is calculated.  The files of the group
are regrouped by their partial hash values.

Partial hash groups that contain only a single file are discarded.

=cut
    
sub compare_by_short_hash
{
    my ($files) = (@_);

    my $hashed_short;
    
    for my $file (@{$files}) {
	my $hash = compute_short_hash($file);
	$file->{SHORTHASH} = $hash;
	push @{$hashed_short->{$hash}}, $file;
    }

    remove_single_entries($hashed_short);

    info 'purged short hash statistics:';
    dump_hash_stats($hashed_short);

    return $hashed_short;
}

=pod

The files of the remaining groups of partial hashes are read again and
hash value over the complete file content is calculated for every
file.  The files are again regrouped, this time by their full hash
value.

Like before, hash groups that contain only a single file are discarded.

=cut
    
sub compare_by_long_hash
{
    my ($hashed_short) = (@_);

    my $hashed_long;

    for my $shorthash (keys %{$hashed_short}) {
	my $files = delete $hashed_short->{$shorthash}; # free memory as early as possible
	for my $file (@{$files}) {
	    my $hash = compute_long_hash($file);
	    $file->{LONGHASH} = $hash;
	    push @{$hashed_long->{$hash}}, $file;
	}
    }

    remove_single_entries($hashed_long);

    info 'purged long hash statistics:';
    dump_hash_stats($hashed_long);

    return $hashed_long;   
}

## TODO: continue POD, reorder contents? (no prototypes anyway)
sub print_duplicates
{
    my ($filegroups) = (@_);

    foreach my $filegroup (values %{$filegroups}) {
	print "$_->{NAME}\n" foreach (@{$filegroup});
	print "\n";
    }
}

sub remove_hardlinked_files
{
    my ($files) = (@_);

    my $by_inode;
    foreach my $file (@{$files}) {
	my $inode = $file->{INODE};
	$by_inode->{$inode} = $file unless exists $by_inode->{$inode};
    }

    return [ values %{$by_inode} ];
}

sub compare_files
{
    my ($files) = (@_);

    infof '%8d files (size %d)', scalar @{$files}, $files->[0]->{SIZE};

    $files = remove_hardlinked_files($files);

    info 'discarded already hardlinked files:';
    infof '%8d files', scalar @{$files};
    return unless @{$files} > 1;

    my $hashed_short = compare_by_short_hash($files);
    return unless %{$hashed_short};
    
    my $hashed_long = compare_by_long_hash($hashed_short);
    $hashed_short = undef; # free memory as early as possible
    return unless %{$hashed_long};
    
    ## TODO    my $deduped = compare_by_content($hashed_long);

    print_duplicates($hashed_long);
}

sub find_matches
{
   foreach my $dev (keys %{$allfiles}) {
       foreach my $size (sort { $b <=> $a } keys %{$allfiles->{$dev}}) {
	   compare_files(delete $allfiles->{$dev}->{$size}); # free memory as early as possible
       }
   }
}

# recursively scan all given directories

info 'dropping sizes with just 1 file';
remove_single_sizes();
dump_stats();

info 'finding matches';
find_matches();
