#!/usr/bin/perl -w
use warnings;
use strict;

package App::Pdupes;

=head1 NAME

pdupes - Perl duplicate file finder and deduper 

=cut

use File::Compare;
use File::Find;
use Digest::xxHash qw(xxhash64);
use Data::Dumper;

use constant {
    LOG_INFO => 0,
    
    HASH_SEED => 0xc001c0de,
    SHORT_HASH_LENGTH => 1024
};

=head1 SYNOPSIS

    pdupes <directory> [<directory> ...]

=head1 DESCRIPTION

pdupes will find duplicate files with identical file content and try
to hardlink them together to save disk space.

=cut
    
# print info text (for debugging)
sub info
{
    return unless LOG_INFO;
    print "@_\n";
}

# print formatted info text (for debugging)
sub infof
{
    return unless LOG_INFO;
    
    my $fmt = shift @_;
    info(sprintf($fmt, @_));
}

# compute and show statistics of global file hash
sub dump_stats
{
    return unless LOG_INFO;

    my ($files) = (@_);
    
    my ($dev_count, $size_count, $file_count) = (0, 0, 0);
    foreach my $dev (keys %{$files}) {
	$dev_count++;
	foreach my $size (keys %{$files->{$dev}}) {
	    $size_count++;
	    $file_count += keys @{$files->{$dev}->{$size}};
	}
    }

    infof '%4d devices %12d sizes %12d files', $dev_count, $size_count, $file_count;
}

# compute and show statistics hashed files
sub dump_hash_stats
{
    return unless LOG_INFO;
    
    my ($hashes) = (@_);
    my ($hash_count, $max_dupes) = (0, 0);

    foreach my $hash (keys %{$hashes}) {
	$hash_count++;
	my $dupes = @{$hashes->{$hash}};
	if ($dupes > $max_dupes) {
	    $max_dupes = $dupes;
	}
    }

    infof '%8d hashes, %8d maximum dupes', $hash_count, $max_dupes;
}

# return the smaller of two values
sub min
{
    my ($a, $b) = (@_);
    if ($a < $b) {
	return $a;
    }
    else {
	return $b;
    }
}

# compute the hash of a file up to a given length
sub compute_hash
{
    my ($file, $maxlen) = (@_);

    my $hash = HASH_SEED;
    my $data;
    
    open HASH, '<', $file->{NAME} or die "can't open `$file->{NAME}': $!";
    my $rest = $maxlen;
    while ($rest) {
	my $to_read = min($rest, $file->{BLKSIZE});
	my $read = read(HASH, $data, $to_read);
	die "read $read bytes, but wanted to read $to_read on `$file->{NAME}': $!" unless $to_read = $read;
	$hash = xxhash64($data, $hash);
	$rest -= $read;
    }
    close HASH or die "can't close `$file->{NAME}': $!";

    return $hash;
}

# compute the short hash of a file
sub compute_short_hash
{
    my ($file) = (@_);

    return compute_hash($file, min($file->{SIZE}, SHORT_HASH_LENGTH));
}

# compute the long hash of a file
sub compute_long_hash
{
    my ($file) = (@_);

    return compute_hash($file, $file->{SIZE});
}

# removes lists with less than 2 elements from a hash of lists
sub remove_single_entries
{
    my ($hash_of_lists) = (@_);

    foreach my $hashKey (keys %{$hash_of_lists}) {
	if (@{$hash_of_lists->{$hashKey}} < 2) {
	    delete $hash_of_lists->{$hashKey};
	}
    }
}

=head1 DETAILED MODE OF OPERATION

pdupes recursively scans all given directories for normal files.
Symlinks, devices, named pipes etc. are skipped.  Empty files are
skipped as well.

Files are grouped by the device their filesystem is on (as hardlinks
can never span multiple filesystems) and their file size (because
identical files must have identical filesizes).

=cut

my $allfiles;
sub register_file
{
    my ($dev, $inode, $mode, $nlink, $uid, $gid, undef, $size, $atime, $mtime, $ctime, $blksize) = lstat($_);

    return unless -f _; # only care about normal files
    return unless $size; # skip empty files

    my $file = {
	DEV => $dev,
	INODE => $inode,
	MODE => $mode,
	NLINK => $nlink,
	UID => $uid,
	GID => $gid,
	SIZE => $size,
	MTIME => $mtime,
	CTIME => $ctime,
	BLKSIZE => $blksize,
	NAME => $File::Find::name
    };

    push @{$allfiles->{$dev}->{$size}}, $file;
}

=pod

File size groups containing only a single file (a unique file size)
are discarded, because there can't be any duplicates.

=cut

sub remove_single_sizes
{
    my ($files) = (@_);

    foreach my $dev (keys %{$files}) {
	remove_single_entries($files->{$dev});
    }

    return $files;
}

=pod

The files of every device/size-group are read and a partial hash value
over the beginning of the file is calculated.  The files of the group
are regrouped by their partial hash values.

Partial hash groups that contain only a single file are discarded.

=cut
    
sub group_by_short_hash
{
    my ($files) = (@_);

    my $hashed_short;
    
    for my $file (@{$files}) {
	my $hash = compute_short_hash($file);
	$file->{SHORTHASH} = $hash;
	push @{$hashed_short->{$hash}}, $file;
    }

    remove_single_entries($hashed_short);

    info 'purged short hash statistics:';
    dump_hash_stats($hashed_short);

    return [ values %{$hashed_short} ];
}

=pod

The files of the remaining groups of partial hashes are read again and
hash value over the complete file content is calculated for every
file.  The files are again regrouped, this time by their full hash
value.

Like before, hash groups that contain only a single file are discarded.

=cut
    
sub group_by_long_hash
{
    my ($files) = (@_);

    my $hashed_long;

    for my $file (@{$files}) {
	my $hash = compute_long_hash($file);
	$file->{LONGHASH} = $hash;
	push @{$hashed_long->{$hash}}, $file;
    }

    remove_single_entries($hashed_long);

    info 'purged long hash statistics:';
    dump_hash_stats($hashed_long);

    return [ values %{$hashed_long} ];
}

## TODO: continue POD, reorder contents? (no prototypes anyway)
sub group_by_content
{
    my ($files) = (@_);

    my $groups;
    
    my $group_id = 0;
    while (@{$files}) {
	
	my $anchor = pop @{$files};
	$groups->{$group_id} = [ $anchor ];

	my $i = 0;
	while ($i < @{$files}) {
	    if (compare( $anchor->{NAME}, $files->[$i]->{NAME} )) {
		$i++;
	    }
	    else {
		push @{$groups->{$group_id}}, splice( @{$files}, $i, 1 );
	    }
	}

	$group_id++;
    }
    
    remove_single_entries($groups);
    
    info 'purged file content statistics:';
    dump_hash_stats($groups);

    return [ values %{$groups} ];
}

sub print_duplicates
{
    my ($files) = (@_);

    print "$_->{NAME}\n" foreach (sort {$a->{NAME} cmp $b->{NAME}} @{$files});
    print "\n";
}

sub remove_already_hardlinked_files
{
    my ($files) = (@_);

    my $by_inode;
    foreach my $file (@{$files}) {
	my $inode = $file->{INODE};
	$by_inode->{$inode} = $file unless exists $by_inode->{$inode};
    }

    return [ values %{$by_inode} ];
}

sub process_long_hash_group
{
    my ($files) = (@_);

    my $content_grouped = group_by_content($files);
    return unless @{$content_grouped};
    
    print_duplicates($_) foreach @{$content_grouped};
}

sub process_short_hash_group
{
    my ($files) = (@_);
    
    my $long_hash_grouped = group_by_long_hash($files);
    return unless @{$long_hash_grouped};

    process_long_hash_group($_) foreach @{$long_hash_grouped};
}

sub process_single_filesize
{
    my ($files) = (@_);

    infof '%8d files (size %d)', scalar @{$files}, $files->[0]->{SIZE};

    my $non_hardlinked_files = remove_already_hardlinked_files($files);
    $files = undef; # free memory as early as possible

    info 'discarded already hardlinked files:';
    infof '%8d files', scalar @{$non_hardlinked_files};
    return unless @{$non_hardlinked_files} > 1;

    my $short_hash_grouped = group_by_short_hash($non_hardlinked_files);
    return unless @{$short_hash_grouped};

    process_short_hash_group($_) foreach @{$short_hash_grouped};
}

sub find_matches
{
   foreach my $dev (keys %{$allfiles}) {
       foreach my $size (sort { $b <=> $a } keys %{$allfiles->{$dev}}) {
	   process_single_filesize(delete $allfiles->{$dev}->{$size}); # free memory as early as possible
       }
   }
}

sub find_files
{
    my (@dirs_to_scan) = (@_);

    $allfiles = {};

    find(\&register_file, @dirs_to_scan);

    return $allfiles;
}

sub run
{
    my $class = shift;
    my (@dirs_to_scan) = (@_);

    die "no directories given" unless @dirs_to_scan;

    $allfiles = find_files(@dirs_to_scan);
    dump_stats($allfiles);

    # recursively scan all given directories

    info 'dropping sizes with just 1 file';
    $allfiles = remove_single_sizes($allfiles);
    dump_stats($allfiles);

    info 'finding matches';
    find_matches();
}

# only do something if called as a script
__PACKAGE__->run(@ARGV) unless defined caller() and caller() ne 'PAR';
